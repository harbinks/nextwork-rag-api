# RAG API with FastAPI

## ğŸ“Œ Project Overview
This project implements a **Retrieval-Augmented Generation (RAG) API** using **FastAPI**, **ChromaDB**, and **Ollama**.  
The API retrieves relevant context from stored documents and uses a local language model to generate accurate, context-aware responses.

---

## ğŸš€ Tech Stack
- FastAPI â€“ API framework  
- Ollama â€“ Local LLM runtime (`tinyllama`)  
- ChromaDB â€“ Vector database for embeddings  
- Python â€“ Core language  

---

## ğŸ§  How the RAG API Works
1. A user sends a query to the API.
2. The query is searched against embeddings stored in ChromaDB.
3. Relevant context is retrieved from the vector database.
4. The context and query are sent to the language model.
5. The model generates a clean and concise answer.
6. The API returns the response in JSON format.

---

## ğŸ“ Project Structure
rag-fastapi/
â”‚â”€â”€ app.py
â”‚â”€â”€ ingest.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ db/
â”‚â”€â”€ data/
â”‚ â””â”€â”€ context.txt


---

## âš™ï¸ Setup Instructions
```bash
1ï¸âƒ£ Create and activate a virtual environment
python -m venv venv
# Windows
.\venv\Scripts\activate
# macOS / Linux
source venv/bin/activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Set up Ollama

Ensure Ollama is running and pull the required model:

ollama pull tinyllama

ğŸ“¥ Ingest Documents

Add context data into ChromaDB:

python ingest.py

â–¶ï¸ Run the API
uvicorn app:app --reload


The API will be available at:

http://127.0.0.1:8000

ğŸ§ª Testing the API
Using Swagger UI

Open in browser:

http://127.0.0.1:8000/docs


Use the /query endpoint to send a question and view the response.

Using PowerShell
Invoke-RestMethod -Uri "http://127.0.0.1:8000/query?q=What is Kubernetes?" -Method Post

âœ… Sample Response
{
  "answer": "Kubernetes is an open-source container orchestration platform used to manage and scale containerized applications."
}

ğŸ¯ Key Learnings

Understanding Retrieval-Augmented Generation (RAG)

Creating and using embeddings for semantic search

Integrating a vector database with a language model

Building APIs with FastAPI

Controlling and cleaning LLM outputs for production-ready responses

ğŸ Conclusion

This project demonstrates a complete RAG pipeline, from document ingestion to context-based answer generation, and provides practical experience in building and testing AI-powered APIs.